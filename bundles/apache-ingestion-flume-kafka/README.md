# Big Data Ingestion with Apache Flume and Apache Kafka

This bundle is a 10 node cluster designed to scale out. Built around Apache
Hadoop components, it contains the following units:

* 1 HDFS Master
* 1 HDFS Secondary Namenode
* 1 YARN Master
* 3 Compute Slaves
* 1 Flume-HDFS
  - 1 Plugin (colocated on the Flume unit)
* 1 Flume-Kafka
* 1 Kafka
* 1 Zookeeper

The Flume-HDFS unit provides an Apache Flume agent featuring an Avro source,
memory channel, and HDFS sink. This agent supports a relation with the
Flume-Kafka charm (`apache-flume-kafka`) to ingest messages published to a
given Kafka topic into HDFS.

## Usage
Deploy this bundle using juju-quickstart:

    juju quickstart u/bigdata-dev/apache-ingestion-flume-kafka

See `juju quickstart --help` for deployment options, including machine
constraints and how to deploy a locally modified version of the
`apache-ingestion-flume` bundle.yaml.

## Configuration
The default Kafka topic where messages are published is unset. Set this to
an existing Kafka topic as follows:

    juju set flume-kafka kafka_topic='<topic_name>'

If you don't have a Kafka topic, you may create one (and verify successful
creation) with:

    juju action do kafka/0 create-topic topic=<topic_name> \
     partitions=1 replication=1
    juju action fetch <id>  # <-- id from above command

You'll also need to specify the Zookeeper connection string for this charm. In
the future, this value will be automatically available via the Kafka relation.
Retrieve the current Zookeeper connection string with:

    juju action do kafka/0 list-zks
    juju action fetch <id>  # <-- id from above command

Set the <ip>:<port> information from the above `zookeepers` output in this
charm:

    juju set flume-kafka zookeeper_connect='<ip:port>'

Once the Flume agents start, messages will start flowing into
HDFS in year-month-day directories here: `/user/flume/flume-kafka/%y-%m-%d`.

## Testing the deployment

### Smoke test HDFS admin functionality
Once the deployment is complete and the cluster is running, ssh to the HDFS
Master unit:

    juju ssh hdfs-master/0

As the `ubuntu` user, create a temporary directory on the Hadoop file system.
The steps below verify HDFS functionality:

    hdfs dfs -mkdir -p /tmp/hdfs-test
    hdfs dfs -chmod -R 777 /tmp/hdfs-test
    hdfs dfs -ls /tmp # verify the newly created hdfs-test subdirectory exists
    hdfs dfs -rm -R /tmp/hdfs-test
    hdfs dfs -ls /tmp # verify the hdfs-test subdirectory has been removed
    exit

### Smoke test YARN and MapReduce
Run the `terasort.sh` script from the Flume unit to generate and sort data. The
steps below verify that Flume is communicating with the cluster via the plugin
and that YARN and MapReduce are working as expected:

    juju ssh flume-hdfs/0
    ~/terasort.sh
    exit

### Smoke test HDFS functionality from user space
From the Flume unit, delete the MapReduce output previously generated by the
`terasort.sh` script:

    juju ssh flume-hdfs/0
    hdfs dfs -rm -R /user/ubuntu/tera_demo_out
    exit

### Smoke test Flume
SSH to the Flume unit and verify the flume-ng java process is running:

    juju ssh flume-hdfs/0
    ps -ef | grep flume-ng # verify process is running
    exit

### Test Kafka-Flume
Generate Kafka messages on the `flume-kafka` unit with the producer script:

    juju ssh flume-kafka/0
    kafka-console-producer.sh --broker-list localhost:9092 --topic <topic_name>
    <type message, press Enter>

To verify these messages are being stored into HDFS, SSH to the `flume-hdfs`
unit, locate an event, and cat it:

    juju ssh flume-hdfs/0
    hdfs dfs -ls /user/flume/flume-kafka  # <-- find a date
    hdfs dfs -ls /user/flume/flume-kafka/yyyy-mm-dd  # <-- find an event
    hdfs dfs -cat /user/flume/flume-kafka/yyyy-mm-dd/FlumeData.[id]


## Scale Out Usage
This bundle was designed to scale out. To increase the amount of Compute
Slaves, you can add units to the compute-slave service. To add one unit:

    juju add-unit compute-slave

You can also add multiple units, for examle, to add four more compute slaves:

    juju add-unit -n4 compute-slave


## Contact Information

- <bigdata-dev@lists.launchpad.net>


## Help

- [Juju mailing list](https://lists.ubuntu.com/mailman/listinfo/juju)
- [Juju community](https://jujucharms.com/community)
